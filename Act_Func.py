import numpy as np
from Activation import Activation

class Sigmoid(Activation):
    def __init__(self):
        # super().__init__(#sigmoid, #derivative of sigmoid)
        pass            
    # I don't think I have to define the backwards and forwards again?
    
class ReLU(Activation):
    def __init__(self):
        # super().__init___(#relu, #derivative of relu)
        pass
    
class Tanh(Activation):
    def __init__(self):
        # super().__init__(blah)
        pass

class Softmax(Activation):
    def __init__(self):
        # super().__init__(blah)
        pass